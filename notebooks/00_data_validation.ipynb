{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85dbc6ce",
   "metadata": {},
   "source": [
    "# 00 Data Validation\n",
    "\n",
    "Validates the canonical processed tables and cross-matches units/traits/items against reference lists. Generates cleaned tables in `data/processed/cleaned/`. No exploratory plotting here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb154fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Detect project root\n",
    "MARKERS = {\"requirements.txt\", \"Projectplan.md\", \".git\"}\n",
    "PROJECT_ROOT = None\n",
    "cwd = Path.cwd()\n",
    "for path in [cwd, *cwd.parents]:\n",
    "    if any((path / m).exists() for m in MARKERS):\n",
    "        PROJECT_ROOT = path\n",
    "        break\n",
    "if PROJECT_ROOT is None:\n",
    "    PROJECT_ROOT = cwd\n",
    "\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "DEFAULT_CANONICAL = DATA_PROCESSED / \"canonical_original\"\n",
    "PROCESSED_DIR = DEFAULT_CANONICAL if DEFAULT_CANONICAL.exists() else DATA_PROCESSED\n",
    "\n",
    "# Use canonical processed reference files (units_s16/items_s16/traits_s16)\n",
    "REF_DIR = PROCESSED_DIR\n",
    "\n",
    "CLEANED_DIR = DATA_PROCESSED / \"cleaned\"\n",
    "INVALID_DIR = DATA_PROCESSED / \"invalid\"\n",
    "CLEANED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "INVALID_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Processed dir: {PROCESSED_DIR}\")\n",
    "print(f\"Cleaned output: {CLEANED_DIR}\")\n",
    "print(f\"Invalid records saved to: {INVALID_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3cbd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load processed tables (canonical outputs)\n",
    "participants = pd.read_csv(PROCESSED_DIR / \"participants.csv\")\n",
    "traits = pd.read_csv(PROCESSED_DIR / \"traits.csv\")\n",
    "units = pd.read_csv(PROCESSED_DIR / \"units.csv\")\n",
    "\n",
    "# Reference lookups from canonical processed reference files\n",
    "units_ref = pd.read_csv(REF_DIR / \"units_s16.csv\")\n",
    "traits_ref = pd.read_csv(REF_DIR / \"traits_s16.csv\")\n",
    "items_ref = pd.read_csv(REF_DIR / \"items_s16.csv\")\n",
    "\n",
    "valid_units = set(units_ref['name'].dropna())\n",
    "valid_traits = set(traits_ref['name_corrected'].dropna()) if 'name_corrected' in traits_ref.columns else set(traits_ref['name'].dropna())\n",
    "\n",
    "valid_items = set()\n",
    "for col in items_ref.columns:\n",
    "    if col == 'name' or col.startswith('comp'):\n",
    "        valid_items.update(items_ref[col].dropna().astype(str).str.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4270ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataset overview\n",
    "print(\"Participants:\", participants.shape)\n",
    "print(\"Traits:\", traits.shape)\n",
    "print(\"Units:\", units.shape)\n",
    "print(\"Matches:\", participants['match_id'].nunique())\n",
    "print(\"Players:\", participants['puuid'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893e6d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Schema checks vs required columns\n",
    "required = {\n",
    "    \"participants\": [\"match_id\", \"puuid\", \"placement\", \"level\", \"last_round\", \"is_win\"],\n",
    "    \"traits\": [\"match_id\", \"puuid\", \"trait_id\", \"num_units\", \"tier_current\"],\n",
    "    \"units\": [\"match_id\", \"puuid\", \"unit_name\", \"unit_tier\", \"rarity\", \"item_0\", \"item_1\", \"item_2\"],\n",
    "}\n",
    "\n",
    "for name, cols in required.items():\n",
    "    df = locals()[name]\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    status = \"OK\" if not missing else f\"MISSING {missing}\"\n",
    "    print(f\"{name}: {status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6397bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Range checks with failures printed\n",
    "failed = {}\n",
    "\n",
    "placement_mask = ~participants['placement'].between(1, 8, inclusive='both')\n",
    "if placement_mask.any():\n",
    "    failed['placement_range'] = participants.loc[placement_mask, ['match_id','puuid','placement']]\n",
    "\n",
    "level_mask = ~participants['level'].between(1, 10, inclusive='both')\n",
    "if level_mask.any():\n",
    "    failed['level_range'] = participants.loc[level_mask, ['match_id','puuid','level']]\n",
    "\n",
    "unit_tier_mask = ~units['unit_tier'].between(1, 3, inclusive='both')\n",
    "if unit_tier_mask.any():\n",
    "    failed['unit_tier_range'] = units.loc[unit_tier_mask, ['match_id','puuid','unit_name','unit_tier']]\n",
    "\n",
    "if not failed:\n",
    "    print(\"[OK] All range checks passed\")\n",
    "else:\n",
    "    for name, df in failed.items():\n",
    "        print(f\"[FAIL] {name}: {len(df)} rows\")\n",
    "        display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde6e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cross-match validity checks and offending rows (traits vs name_corrected; items vs item names or components or observed items)\n",
    "item_cols = [c for c in units.columns if c.startswith('item_')]\n",
    "items_long = units[['match_id','puuid','unit_name'] + item_cols].set_index(['match_id','puuid','unit_name']).stack(dropna=True).reset_index()\n",
    "items_long.columns = ['match_id','puuid','unit_name','slot','item']\n",
    "items_long['item'] = items_long['item'].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "observed_items = set(items_long['item'].dropna())\n",
    "valid_items_all = valid_items.union(observed_items)\n",
    "\n",
    "invalid_units = units[~units['unit_name'].isin(valid_units)]\n",
    "invalid_traits = traits[~traits['trait_id'].isin(valid_traits)]\n",
    "invalid_items_rows = items_long[~items_long['item'].isin(valid_items_all)]\n",
    "\n",
    "print(f\"Invalid units: {len(invalid_units)} rows\")\n",
    "if len(invalid_units):\n",
    "    display(invalid_units[['match_id','puuid','unit_name']])\n",
    "\n",
    "print(f\"Invalid traits (compared to name_corrected): {len(invalid_traits)} rows\")\n",
    "if len(invalid_traits):\n",
    "    display(invalid_traits[['match_id','puuid','trait_id']])\n",
    "\n",
    "print(f\"Invalid items (slot entries not matching item names/components/observed set): {len(invalid_items_rows)} rows\")\n",
    "if len(invalid_items_rows):\n",
    "    display(invalid_items_rows[['match_id','puuid','unit_name','slot','item']])\n",
    "\n",
    "# Save invalids for manual review\n",
    "if len(invalid_units):\n",
    "    invalid_units.to_csv(INVALID_DIR / \"invalid_units.csv\", index=False)\n",
    "if len(invalid_traits):\n",
    "    invalid_traits.to_csv(INVALID_DIR / \"invalid_traits.csv\", index=False)\n",
    "if len(invalid_items_rows):\n",
    "    invalid_items_rows.to_csv(INVALID_DIR / \"invalid_items.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dc8a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cleaning rules (based on checks):\n",
    "# - Drop unit rows with unit_tier outside 1-3.\n",
    "# - Drop unit rows with unknown unit_name.\n",
    "# - Trim item slots; keep items if in reference (full names/components) or observed set, else set to NA.\n",
    "# - Drop trait rows with unknown trait_id (matched against name_corrected list).\n",
    "# - Keep participants/traits tied to players that still have at least one unit after cleaning.\n",
    "\n",
    "units_clean = units.copy()\n",
    "units_clean = units_clean[units_clean['unit_tier'].between(1,3, inclusive='both')]\n",
    "units_clean = units_clean[units_clean['unit_name'].isin(valid_units)].copy()\n",
    "\n",
    "for col in [c for c in units_clean.columns if c.startswith('item_')]:\n",
    "    units_clean[col] = units_clean[col].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    mask_valid = units_clean[col].isin(valid_items_all) | units_clean[col].isna()\n",
    "    units_clean.loc[~mask_valid, col] = pd.NA\n",
    "\n",
    "traits_clean = traits[traits['trait_id'].isin(valid_traits)].copy()\n",
    "\n",
    "valid_players = units_clean[['match_id','puuid']].drop_duplicates()\n",
    "participants_clean = participants.merge(valid_players, on=['match_id','puuid'], how='inner')\n",
    "traits_clean = traits_clean.merge(valid_players, on=['match_id','puuid'], how='inner')\n",
    "\n",
    "# Save cleaned tables\n",
    "p_out = DATA_PROCESSED / \"cleaned\" / \"participants.csv\"\n",
    "t_out = DATA_PROCESSED / \"cleaned\" / \"traits.csv\"\n",
    "u_out = DATA_PROCESSED / \"cleaned\" / \"units.csv\"\n",
    "participants_clean.to_csv(p_out, index=False)\n",
    "traits_clean.to_csv(t_out, index=False)\n",
    "units_clean.to_csv(u_out, index=False)\n",
    "\n",
    "print(\"Saved cleaned tables:\")\n",
    "print(p_out)\n",
    "print(t_out)\n",
    "print(u_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f19de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Post-clean counts\n",
    "print(\"Participants (clean):\", len(participants_clean))\n",
    "print(\"Traits (clean):\", len(traits_clean))\n",
    "print(\"Units (clean):\", len(units_clean))\n",
    "print(\"Matches (clean):\", participants_clean['match_id'].nunique())\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
